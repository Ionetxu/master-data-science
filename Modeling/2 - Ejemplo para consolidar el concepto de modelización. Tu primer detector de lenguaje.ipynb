{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ejercicio: Un sencillo detector de idioma en documentos\n",
    "\n",
    "Vamos a realizar otro ejercicio de modelización, ahora con un caso de uso de reconocimiento de patrones: distinguir si un documento está escrito en inglés o en español.\n",
    "\n",
    "(No vamos a usar técnicas de NLP \"de verdad\", sino conceptos básicos para seguir ilustrando la filosofía de la modelización. Más adelante tenéis un curso de NLP). \n",
    "\n",
    "Recordemos que con **modelización** nos referimos a adoptar una metodología para tomar decisiones automáticas basadas en datos. \n",
    "\n",
    "Que un modelo tiene: \n",
    "\n",
    "* Unos parámetros\n",
    "* Un proceso para \"ajustar\" los parámetros\n",
    "* Un proceso para aplicar la decisión a nuevos datos\n",
    "\n",
    "Y que:\n",
    "\n",
    "* En muchos casos el modelo es un resumen de los datos. \n",
    "* Si los datos usados para ajustar cambian, la decisión debería cambiar (esto es bueno!)\n",
    "* Debería mejorar conforme más datos se usan para ajustar\n",
    "* Está basado en suposiciones!! Un modelo es como mínimo igual de bueno que las suposiciones que se han realizado para llevarlo a cabo. \n",
    "\n",
    "Este ejercicio nos permitirá consolidar todos esos conceptos. \n",
    "\n",
    "## Ejercicio\n",
    "\n",
    "¿Cómo diseñarías una función simple que identificase si una cadena de texto está en inglés o en español? (Esto es un problema habitual en redes sociales o gestión documental, entre otros). \n",
    "\n",
    "Vamos a usar un pequeño dataset derivado de https://github.com/FerreroJeremy/Cross-Language-Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdataset-lang/\u001b[00m\r\n",
      "├── \u001b[01;34mfiles\u001b[00m [200 entries exceeds filelimit, not opening dir]\r\n",
      "├── test-en.txt\r\n",
      "├── test-es.txt\r\n",
      "├── training-en.txt\r\n",
      "└── training-es.txt\r\n",
      "\r\n",
      "1 directory, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "#!tree --filelimit 5 dataset-lang/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un directorio \"files\" con los documentos. Y unos ficheros *.txt de listas. \n",
    "\n",
    "Estos ficheros sirven para 2 cosas: especifican qué documentos están en inglés y cuáles en español (veremos el concepto de \"etiqueta\"). \n",
    "\n",
    "También los hemos dividido a mano en los documentos que usaremos para ajustar el modelo (training) y los que después usaremos para simular cómo de bien o mal habría funcionado (test). Esta partición es una práctica habitual en modelización, sobre todo en machine learning. \n",
    "\n",
    "Imaginemos cómo se podría detectar un idioma en un documento mediante algún criterio práctico y sencillo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(sentence):\n",
    "     \n",
    "    votes_english = 0 \n",
    "    votes_spanish = 0\n",
    "    \n",
    "    if \"ñ\" in sentence:\n",
    "        votes_spanish += 100\n",
    "            \n",
    "    for s in sentence.split(\" \"):\n",
    "        \n",
    "        if s in ['the', 'this', 'in', 'where', 'take']:\n",
    "            votes_english += 1\n",
    "        if s in ['el', 'la', 'este', 'esta', 'en', 'que']:\n",
    "            \n",
    "            votes_spanish += 1\n",
    "                    \n",
    "    if votes_spanish == votes_english:\n",
    "        return None\n",
    "    elif votes_spanish > votes_english:\n",
    "        return 'es'\n",
    "    else:\n",
    "        return 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 dataset-lang/training-es.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a probar esta en un fichero. Aquí una función que convierte el fichero a cadena y le aplica el criterio que hemos programado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "FILE_ES = \"dataset-lang/files/ep-00-01-17-es.txt\"\n",
    "\n",
    "def file2str(file):\n",
    "\n",
    "    with open(file,  encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        str = \"\"\n",
    "        for s in lines:\n",
    "            str += re.sub( '\\s+', ' ', s.lower().replace(\"\\n\", \"\") ).strip()\n",
    "        #print(\"Read\", file, \"\\nnchars =\", len(str), file=sys.stderr)\n",
    "    \n",
    "    return str\n",
    "        \n",
    "detect_lang(file2str(FILE_ES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_lang('Here is the document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, hemos tomado una decisión automáticamente, pero está basada en conocimiento a priori y manualmente codificado. \n",
    "\n",
    "Vamos a ver ahora una estrategia basada en datos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "PREFIX = \"dataset-lang/files/\"\n",
    "\n",
    "def collect_words(lang):\n",
    "\n",
    "    # lang is \"es\" or \"en\"\n",
    "    \n",
    "    words = dict()\n",
    "    \n",
    "    # Collect words into dict\n",
    "    filelist = \"dataset-lang/training-\" + lang + \".txt\"\n",
    "    with open(filelist) as f:\n",
    "        \n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            realfile = PREFIX + l.replace(\"\\n\", \"\")\n",
    "            str_ = file2str(realfile)\n",
    "            words_in_file  = str_.split()\n",
    "            for w in words_in_file:\n",
    "                if w in words:\n",
    "                    words[w] += 1\n",
    "                else:\n",
    "                    words[w] = 1\n",
    "            \n",
    "    print(\"nwords = \", len(words), file=sys.stderr)\n",
    "    \n",
    "    # Finished collecting, keep top 50 words\n",
    "    values = -np.array(list(words.values()))\n",
    "    keys = np.array(list(words.keys()))\n",
    "\n",
    "    selected = np.argpartition(values, 50)[:50]\n",
    "    return keys[selected], values[selected]\n",
    "        \n",
    "    \n",
    "def vote(sentence, dict_es, dict_en):\n",
    "    \n",
    "    score_en = 0\n",
    "    score_es = 0\n",
    "    \n",
    "    for w in sentence.split(\" \"):\n",
    "        if w in dict_es:\n",
    "            score_es +=1\n",
    "        if w in dict_en:\n",
    "            score_en += 1\n",
    "            \n",
    "    return score_es, score_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nwords =  248024\n",
      "nwords =  180751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w_es, _ = collect_words(\"es\")\n",
    "w_en, _ = collect_words(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la salida del detector en un par de frases de ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote(\"a ver en que idioma detecta esta frase\", w_es, w_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote(\"now let's try a text in english\", w_es, w_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque lo que se suele hacer es medir el porcentaje de acierto en otro conjunto de muestras, al que se suele llamar \"conjunto de test\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LIST_ES = 'dataset-lang/test-es.txt'\n",
    "\n",
    "n_es_correct = 0\n",
    "n = 0\n",
    "with open(TEST_LIST_ES) as f:\n",
    "        \n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        realfile = PREFIX + l.replace(\"\\n\", \"\")\n",
    "        str_ = file2str(realfile)\n",
    "        result = vote(str_, w_es, w_en)\n",
    "        if result[0] > result[1]:\n",
    "            n_es_correct += 1\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_es_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque es un ejemplo sencillo, esta solución es más elegante que las reglas anteriores, porque no hemos diseñado reglas por prueba y error. Al contrario, hemos \"ajustado\" el modelo \"enseñándole\" cómo son documentos en inglés y en español. La metodología consiste en detectar las palabras más frecuentes durante el ajuste y para aplicar a nuevos documentos, se realiza un proceso de votación. \n",
    "\n",
    "Podemos decir que el ajuste es la fase en la que \"enseñamos\" al sistema lo que tiene que saber para hacer la tarea. \n",
    "\n",
    "Además, vamos a evaluar los puntos de antes: \n",
    "\n",
    "* **¿El modelo es un resumen de los datos?** Sí. Fijémonos que cuando ya está \"ajustado\", sólo necesitamos las listas de palabras, no los documentos enteros. \n",
    "\n",
    "* **¿Si los datos cambian, la decisión cambia?** Sí.: Si en lugar de artículos fuesen tweets. Fijemonos que exactamente el mismo sistema podría usarse para distinguir inglés de francés. Ojo: necesitaremos datos de documentos en francés. \n",
    "\n",
    "* **¿Mejora con más datos?**: Sí. Cuantos más datos, más fiables las palabras más frecuentes.  \n",
    "\n",
    "* **¿Está basado en suposiciones?** Sí. (Por ejemplo, que los documentos de aprendizaje no tienen mezcla de idiomas, que están bien \"tokenizados\", que son suficientemente largos, etc). \n",
    "\n",
    "\n",
    "\n",
    "Aquí ya podemos empezar a adelantar terminología de metodología general de modelización, que se explican durante clase:\n",
    "\n",
    "* Variables (características, atributos, variables dependientes, entradas). \n",
    "* \"Fit\": Ajustar el modelo (entrenar, aprender)\n",
    "* \"Predict\": Aplicar el modelo (testear, predecir, inferencia)\n",
    "* Conjunto de entrenamiento y conjunto de test\n",
    "* Representación\n",
    "* Etiquetas (\"targets\", variable dependiente, salidas) \n",
    "* Hiperparámetros\n",
    "* Evaluación\n",
    "* (No tratado) Función objetivo\n",
    "\n",
    "<br>\n",
    "**Take-home messages**\n",
    "\n",
    "Hemos visto un ejemplo (muy simple, hello world!) de modelos. Lo importante a retener es que\n",
    "\n",
    "* De nuevo, un modelo es una metodología para tomar decisiones automáticas a partir de datos (y suposiciones). Dados unos datos (training set), \"aprende\" la información necesaria para realizar deducciones (a veces se habla de predicciones) sobre nuevos datos. Terminología se habla de ajustar un modelo y aplicar el modelo. \n",
    "\n",
    "* Si se hace bien, es una solución más flexible que programar los criterios a mano. \n",
    "\n",
    "* Sin datos no hay modelo. Cuanto mejor sean los datos, mejor será el modelo. \n",
    "\n",
    "* Hay que verificar las suposiciones continuamente.\n",
    "\n",
    "* Se han aprendido nuevos términos: conjunto de entrenamiento, de test, representación, etiquetas, hiperparámetros. \n",
    "\n",
    "\n",
    "<br>\n",
    "<div style=\"background-color:orange;border-radius:20px;padding:20px\">\n",
    "Atención!!! A veces, sobre todo en el contexto de modelos de machine learning, se habla de estos modelos como \"sistemas a los que no hay que programar, ya que se aprenden a programar ellos solos a partir de los datos\". Es una afirmación peligrosa si se omiten los requisitos: no es que no se programen, sino que no se programan unas reglas manuales. Lo que se programa es la capacidad del sistema para ajustar y aplicar un modelo en base a unos datos. También tiene que quedar claro que los datos son toda la información necesaria para ajustar el modelo. Por ejemplo, en este caso: los documentos y sus etiquetas. Los documentos sin etiquetas son insuficientes para que un sistema aprenda.   \n",
    "     \n",
    "    </b>\n",
    "</div>\n",
    " \n",
    "<br>\n",
    "Ver por ejemplo el post de [software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35) (es controversial). \n",
    "<br><br>\n",
    "\n",
    "<div style=\"background-color:orange;border-radius:20px;padding:20px\">\n",
    "<p>Atención!!!El ejemplo es muy sencillo a propósito. Aunque funciona relativamente bien, los profesionales que necesitan detectar idiomas usan métodos más complejos como clasificadores con redes neuronales. \n",
    "<p>Aunque también es cierto que los profesionales de data science aprenden con el tiempo que si un método sencillo es good enough, adelante. \n",
    "    </b>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
